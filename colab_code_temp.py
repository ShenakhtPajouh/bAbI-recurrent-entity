# -*- coding: utf-8 -*-
"""preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OV8xz6tWnU57DCaoNix1hm8QBC3lhWpS
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import numpy as np
import pickle
import time
import matplotlib.pyplot as plt

import sys
sys.path.append('drive/My Drive/Colab Notebooks/bAbI')

import tokenization

import glob
import pickle
!ls '/content/drive/My Drive/BERT/'
vocab_path=glob.glob("/content/drive/My Drive/BERT/vocab.txt")
print("r",vocab_path)
for file in vocab_path:
  with open(file,"r") as vocab:
    j=0
    for line in vocab:
#       print(line)
      if j==3:
        break
      j=j+1
  
with open('/content/drive/My Drive/BERT/vocab.txt',"r") as vocab:
    j=0
    for line in vocab:
#       print(line)
      if j==3:
        break
      j=j+1
Data_path="/content/drive/My Drive"
file=open(Data_path+'/BERT/embeddings_table.pkl',"rb")
embeddings=pickle.load(file)
for line in embeddings:
#   print(line[0])
  if j==0:
    break

vocab=open('/content/drive/My Drive/BERT/vocab.txt')
start=0
end=0
i=0
print('hey')
for line in vocab:
#   print(line)
  if line.strip()=='the':
    start=i
  if line.strip()=='necessitated':
    end=i
  if line.strip()=='!':
    print(i)
    st=i
  if line.strip()=='?':
    print(i)
    q=i
  if line.strip()=='[UNK]':
    unk_ind=i
  i=i+1
print("start:",start)
print("end:",end)
print("!",st)
print("?",q)
print('unk',unk_ind)

vocab=open('/content/drive/My Drive/BERT/vocab.txt')
new_vocab=""
dictionary={}
i=0
j=0
for line in vocab:
  if j==0 or line.strip()=='[UNK]' or line.strip()=='[CLS]' or line.strip()=='[SEP]' or line.strip()=='[MASK]':
    new_vocab=new_vocab+line.strip()+"\n"
    dictionary[j]=line.strip()
    j=j+1
  if i>=st and i<=q:
    new_vocab=new_vocab+line.strip()+"\n"
    dictionary[j]=line.strip()
    j=j+1
#   print(i)
  if i>=start and i<=end:
    new_vocab=new_vocab+line.strip()+"\n"
    dictionary[j]=line.strip()
    j=j+1
  i=i+1
print("vocab size:",j)
print(new_vocab)
# with open('/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/vocab.pkl','wb') as file:
#   pickle.dump(new_vocab,file)
f=open('/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/vocab.txt','w')
f.write(new_vocab)
f.close()
with open('/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/dictionary.pkl','wb') as file:
  pickle.dump(dictionary,file)
with open('/content/drive/My Drive/BERT/embeddings_table.pkl','rb') as file:
  embedding_table=pickle.load(file)
  print(type(embedding_table))
embedding_matrix=np.concatenate([np.expand_dims(embedding_table[0],axis=0),embedding_table[unk_ind:unk_ind+4],
                                 embedding_table[st:q+1],
                                 embedding_table[start:end+1]],axis=0)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/embedding_matrix.pkl','wb') as file:
  pickle.dump(embedding_matrix,file)

if embedding_matrix[0].all()==embedding_table[1996].all():
  print('true')

def preprocess(file):
    """
    Description:
     this function get text files as input and outputs triples (context, question, answer)

     inputs:
     file : a .txt file that contains train samples for all 20 tasks

     :returns:
        data: a list of [context, question, answer, support]s , context: a list of sentences
     """
    
    with open(file, "r", encoding="utf8") as file:
        data=[]
        context=[]
#         j=0
        for line in file:
            line=line.lower()
#             if j==15:
#               break
#             j=j+1
            number, text = tuple(line.strip().split(" ", 1))
            if number is "1":
                ''' new context '''
                context = []
            if "\t" in text:
#                 print(number, text, text.split("\t"),*context)
                '''Tabs are the separator between questions and answers, and are not present in context statements'''
                question, answer, support = tuple(text.split("\t"))
                if len(answer.split(","))==1: 
                    ''' we only accept those that have 1-word answer'''
                    data.append([context.copy(),question, answer,[int(s) for s in support.split()]])
            else:
                context.append(text)
        with open("/content/drive/My Drive/Colab Notebooks/bAbI/qa1_train_data.pkl", "wb") as data_out:
          pickle.dump(data,data_out)

def tokenize(data, vocab_path):
  tokenized_data=[]
  max_sent_num=0
  max_sent_len=0
  unk=0
#   max_question_len=0
  tokenizer= tokenization.FullTokenizer(vocab_path)
  for context, question, answer, support_list in data:
    if len(context)>max_sent_num:
      max_sent_num=len(context)
    context_ids=[] 
    for sentence in context:
      ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))
      context_ids.append(ids)
      if len(ids)>max_sent_len:
        max_sent_len=len(ids)
    question_ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(question))
    if '[UNK]' in tokenizer.tokenize(sentence) or '[UNK]' in tokenizer.tokenize(question):
      unk=unk+1
    if len(question_ids)>max_sent_len:
      max_sent_len=len(question_ids)
#     if len(question_ids)>max_question_len:
#       max_question_len=len(question_ids)
#       ques=question
    answer_ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(answer))
    tokenized_data.append([context_ids, question_ids, answer_ids, support_list])
  print('unk',unk)
  return tokenized_data, max_sent_num, max_sent_len

preprocess_start=time.time()
preprocess("/content/drive/My Drive/Colab Notebooks/bAbI/qa1_single-supporting-fact_train.txt")
preprocess_end=time.time()
preprocess_duration=preprocess_end-preprocess_start
print(preprocess_duration)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/qa1_train_data.pkl","rb") as file:
  data=pickle.load(file)
tokenize_start=time.time()
tokenized_data,max_sent_num, max_sent_len=tokenize(data,'/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/vocab.txt')
tokenize_end=time.time()
tokenize_duration= tokenize_end-tokenize_start
print(tokenize_duration)
print(len(tokenized_data),data[-1], max_sent_num, max_sent_len)
# print(ques)
print(tokenized_data[-1])
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_tokenized_data.pkl","wb") as file:
  pickle.dump(tokenized_data, file)
config_dic={}
config_dic['paragraphs_num']=len(tokenized_data)
config_dic['max_sent_num']=max_sent_num
config_dic['max_sent_len']=max_sent_len
config_dic['batch_size']=32
config_dic['epochs_num']=200
config_dic['initial_lr']=0.01
config_dic['max_entity_num']=20
config_dic['vocab_size']=30522
config_dic['embedding_dim']=100
config_dic['val_split']=0.125
with open('/content/drive/My Drive/Colab Notebooks/bAbI/config.pkl','wb') as file:
  pickle.dump(config_dic,file)

def convert_to_tensors(tokenized_data, max_sent_num, max_sent_len):
  prgrphs_num=len(tokenized_data)
  print('prgrphs_num', len(tokenized_data))
  paragraphs=np.zeros(shape=[prgrphs_num, max_sent_num, max_sent_len],dtype=np.int32)
  paragraphs_mask=np.zeros(shape=[prgrphs_num, max_sent_num, max_sent_len],dtype=np.bool)
  questions=np.zeros(shape=[prgrphs_num, max_sent_len], dtype=np.int32)
  answers=np.zeros(shape=[prgrphs_num, 1],dtype=np.int32)
  i=0
  for prgrph, question, answer, support_list in tokenized_data:
    questions[i,:len(question)]=np.asarray(question)
    answers[i]=np.asarray(answer)
    for j in range(len(prgrph)):
      sentence=prgrph[j]
      paragraphs[i,j,:len(sentence)]=np.asarray(sentence)
      paragraphs_mask[i,j,:len(sentence)]=np.ones(shape=[len(sentence)],dtype=np.bool)
    i=i+1
    
  return paragraphs, paragraphs_mask, questions, answers

paragraphs, paragraphs_mask, questions, answers=convert_to_tensors(tokenized_data, max_sent_num, max_sent_len)
print(paragraphs.shape, paragraphs_mask.shape, questions.shape, answers.shape)
print(paragraphs[-1],paragraphs_mask[-1], questions[-1],answers[-1])
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_paragraphs.pkl","wb") as file:
  pickle.dump(paragraphs, file)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_paragraphs_mask.pkl","wb") as file:
  pickle.dump(paragraphs_mask, file)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_questions.pkl","wb") as file:
  pickle.dump(questions, file)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_answers.pkl","wb") as file:
  pickle.dump(answers, file)

import spacy

def extract_keys(paragraphs,dictionary, n=20):
  
# SUBJECTS = ["nsubj", "nsubjpass", "csubj", "csubjpass", "agent", "expl"]
# OBJECTS = ["dobj", "dative", "attr", "oprd"]

  keys_tags = ["nsubj", "nsubjpass", "csubj", "csubjpass", "agent", "expl","dobj", "dative", "attr", "oprd","pobj"]
  parser=spacy.load('en')
  keys=np.zeros([len(paragraphs),n],np.int32)
  keys_mask=np.zeros([len(paragraphs),n],np.bool)
  j=0
  for prgrph in paragraphs:
    words_ind=set()
    doc=parser(prgrph)
    for i,tok in enumerate(doc):
      if tok.dep_ in keys_tags:
        words_ind.add(dictionary[str(tok)])
    keys[j,:len(words_ind)]= list(words_ind)
    keys_mask[j,:len(words_ind)]=np.ones([len(words_ind)],np.bool)
    j=j+1
    if j%1000==0:
      print('j:',j)
    
  return keys,keys_mask

with open('/content/drive/My Drive/Colab Notebooks/bAbI/qa1_train_data.pkl','rb') as file:
  data=pickle.load(file)
contexts=[]
for context, questiion, answer, _ in data:
  contexts.append(" ".join(context))
with open('/content/drive/My Drive/Colab Notebooks/bAbI/contexts.pkl', 'wb') as file:
  pickle.dump(contexts, file)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/dictionary.pkl','rb') as file:
  dictionary=pickle.load(file)
dic_rev={}
for ind,word in dictionary.items():
  dic_rev[word]=ind
with open('/content/drive/My Drive/Colab Notebooks/bAbI/word_to_id.pkl','wb') as file:
  pickle.dump(dic_rev,file)
print(dic_rev['mary'])
print(contexts[:3])
text=['mary got the milk there. john moved to the bedroom. sandra went back to the kitchen. mary travelled to the hallway. john got the football there. john went to the hallway. john put down the football. mary went to the garden. john went to the kitchen. sandra travelled to the hallway. daniel went to the hallway. mary discarded the milk']

with open('/content/drive/My Drive/Colab Notebooks/bAbI/contexts.pkl','rb') as file:
  contexts=pickle.load(file)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/word_to_id.pkl','rb') as file:
  word_to_id=pickle.load(file)
start_time=time.time()
keys,keys_mask=extract_keys(contexts, word_to_id)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/keys.pkl','wb') as file:
  pickle.dump(keys,file)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/keys_mask.pkl','wb') as file:
  pickle.dump(keys_mask,file)
print(keys[:5],keys_mask[:5])
end_time=time.time()
duration=end_time-start_time
print("duration:", duration)

import tensorflow as tf
import numpy as np
from tensorflow.contrib import autograph
from tensorflow.contrib import layers
from functools import partial
K = tf.keras.backend
def prelu(features, alpha, scope=None):
    """
    Implementation of [Parametric ReLU](https://arxiv.org/abs/1502.01852) borrowed from Keras.
    """
    with tf.variable_scope(scope, 'PReLU'):
        pos = tf.nn.relu(features)
        neg = alpha * (features - tf.abs(features)) * 0.5
    return pos + neg


class Sent_encoder(tf.keras.layers.Layer):
    def __init__(self, name=None):
        if name is None:
            name = 'sent_encoder'
        super().__init__(name=name)
        self.positional_mask=None
        self.built=False

    def build(self, input_shape):
        input_shapee=input_shape.as_list()
        print(input_shapee)
        print(type(input_shapee[1]))
        self.positional_mask=self.add_weight(shape=[input_shapee[1],input_shapee[2]],name='positional_mask')
        self.built=True

    def call(self, inputs):
        """
        Description:
            encode given sentences with weigthed bag of words algorithm
        Args:
            input: sents shape: [current_prgrphs_num,max_sent_len,embedding_dim]
            output: encoded sentences of shape [current_prgrphs_num,encoding_dim] , here encoding_dim is equal to embedding_dim
        """
        ' I assume word embedding for indexes greater that sentnece length is zero vector, so it does not effect sentence encoding '
        print("positional",self.positional_mask.shape)
        return tf.reduce_sum(tf.multiply(inputs,tf.math.l2_normalize(self.positional_mask)),axis=1)



class EntityCell(tf.keras.layers.Layer):
    """
    Entity Cell.
    call with inputs and keys
    """

    def __init__(self, max_entity_num, entity_embedding_dim, activation=tf.nn.relu, name=None,
                 **kwargs):
        if name is None:
            name = 'Entity_cell'
        super().__init__(name=name)
        self.max_entity_num = max_entity_num
        self.entity_embedding_dim = entity_embedding_dim
        self.activation = activation
        # if initializer is None:
        #     self.initializer = tf.keras.initializers.random_normal()

        self.U = None
        self.V = None
        self.W = None
        self.built = False

    def build(self, input_shape):
        shape = [self.entity_embedding_dim, self.entity_embedding_dim]
        self.U = self.add_weight(shape=shape, name='U')
        self.V = self.add_weight(shape=shape, name='V')
        self.W = self.add_weight(shape=shape, name='W')
        self.built = True

    def get_gate(self, encoded_sents, current_hiddens, current_keys):
        """
        Description:
            calculate the gate g_i for all hiddens of given paragraphs
        Args:
            inputs: encoded_sents of shape: [current_prgrphs_num, encoding_dim]
                    current_hiddens: [current_prgrphs_num, entity_num, entity_embedding_dim]
                    current_keys: [current_prgrphs_num, entity_num, entity_embedding_dim]

            output: gates of shape : [curr_prgrphs_num, entity_num]
        """

        print('enocded_sents dtype:', encoded_sents.dtype)
        print(current_keys.dtype)
        print('current_hiddens dtype:', current_hiddens.dtype)
        print('enocded_sents shape:', tf.shape(encoded_sents))
        return tf.sigmoid(tf.reduce_sum(tf.multiply(tf.expand_dims(encoded_sents, 1), current_hiddens) +
                                        tf.multiply(tf.expand_dims(encoded_sents, 1), current_keys), axis=2))

    def update_hidden(self, gates, current_hiddens, current_keys, encoded_sents):
        """
        Description:
            updates hidden_index for all prgrphs
        Args:
            inputs: gates shape: [current_prgrphs_num, entity_num]
                    encoded_sents of shape: [current_prgrphs_num, encoding_dim]
                    current_hiddens: [current_prgrphs_num, entity_num, entity_embedding_dim]
                    current_keys: [current_prgrphs_num, entity_num, entity_embedding_dim]
        """
        curr_prgrphs_num = tf.shape(current_hiddens)[0]
        h_tilda = self.activation(
            tf.reshape(tf.matmul(tf.reshape(current_hiddens, [-1, self.entity_embedding_dim]), self.U) +
                       tf.matmul(tf.reshape(current_keys, [-1, self.entity_embedding_dim]), self.V) +
                       tf.matmul(tf.reshape(tf.tile(tf.expand_dims(encoded_sents, 1), [1, self.max_entity_num, 1]),
                                            shape=[-1, self.entity_embedding_dim]), self.W),
                       shape=[curr_prgrphs_num, self.max_entity_num, self.entity_embedding_dim]))
        'h_tilda shape: [current_prgrphs_num, entity_num, entity_embedding_dim]'
        # tf.multiply(gates,h_tilda)
        print("gates shape:", tf.shape(gates))
        updated_hiddens = current_hiddens + tf.multiply(
            tf.tile(tf.expand_dims(gates, axis=2), [1, 1, self.entity_embedding_dim]), h_tilda)

        return updated_hiddens

    def normalize(self, hiddens):
        return tf.nn.l2_normalize(hiddens, axis=2)

    def call(self, inputs, prev_states, keys, use_shared_keys=False, **kwargs):
        """

        Args:
            inputs: encoded_sents of shape [batch_size, encoding_dim] , batch_size is equal to current paragraphs num
            prev_states: tensor of shape [batch_size, key_num, dim]
            keys: tensor of shape [batch_size, key_num, dim] if use_shared_keys is False and
                  [key_num, dim] if use_shared_keys is True
            use_shared_keys: if it is True, it use shared keys for all samples.

        Returns:
            next_state: tensor of shape [batch_size, key_num, dim]
        """

        encoded_sents = inputs
        gates = self.get_gate(encoded_sents, prev_states, keys)
        updated_hiddens = self.update_hidden(gates, prev_states, keys, encoded_sents)
        return self.normalize(updated_hiddens)

    def get_initial_state(self):
        return tf.zeros([self.max_entity_num, self.entity_embedding_dim], dtype=tf.float32)

    # def __call__(self, inputs, prev_state, keys, use_shared_keys=False, **kwargs):
    #     """
    #     Do not fill this one
    #     """
    #     return super().__call__(inputs=inputs, prev_state=prev_state, keys=keys,
    #                             use_shared_keys=use_shared_keys, **kwargs)


# @autograph.convert()
def simple_entity_network(inputs, keys, entity_cell=None,
                          initial_entity_hidden_state=None,
                          use_shared_keys=False, return_last=True):
    """
    Args:
        entity_cell: the EntityCell
        inputs: a list containing a tensor of shape [batch_size, seq_length, dim] and its mask of shape [batch_size, seq_length]
                batch_size=current paragraphs num, seq_length=max number of senteces
        keys: tensor of shape [batch_size, key_num, dim] if use_shared_keys is False and
                  [key_num, dim] if use_shared_keys is True
        use_shared_keys: if it is True, it use shared keys for all samples.
        mask_inputs: tensor of shape [batch_size, seq_length] and type tf.bool
        initial_entity_hidden_state: a tensor of shape [batch_size, key_num, dim]
        return_last: if it is True, it returns the last state, else returns all states

    Returns:
        if return_last = True then a tensor of shape [batch_size, key_num, dim] (entity_hiddens)
        else of shape [batch_size, seq_length+1 , key_num, dim] it includes initial hidden states as well as states for each step ,total would be seq_len+1
    """

    encoded_sents, mask = inputs
    print("type mask", type(mask))
    # print("encoded_sents shape:", encoded_sents.shape)
    seq_length = tf.shape(encoded_sents)[1]
    batch_size = tf.shape(encoded_sents)[0]
    key_num = tf.shape(keys)[1]
    entity_embedding_dim = tf.shape(keys)[2]

    if entity_cell is None:
        entity_cell = EntityCell(max_entity_num=key_num, entity_embedding_dim=entity_embedding_dim,
                                 name='entity_cell')

    if initial_entity_hidden_state is None:
        initial_entity_hidden_state = tf.tile(tf.expand_dims(entity_cell.get_initial_state(), axis=0),
                                              [batch_size, 1, 1])
    if return_last:
        entity_hiddens = initial_entity_hidden_state
    else:
        print("return_lastttttttttt:", return_last)
        all_entity_hiddens = tf.expand_dims(initial_entity_hidden_state, axis=1)

    def cond(encoded_sents, mask, keys, entity_hiddens, i, iters):
        return tf.less(i, iters)

    def body_1(encoded_sents, mask, keys, entity_hiddens, i, iters):
        indices = tf.where(mask[:, i])
        indices = tf.cast(tf.squeeze(indices, axis=1), tf.int32)
        curr_encoded_sents = tf.gather(encoded_sents[:, i, :], indices)
        curr_keys = tf.gather(keys, indices)
        prev_states = tf.gather(entity_hiddens, indices)
        updated_hiddens = entity_cell(curr_encoded_sents, prev_states, curr_keys)
        entity_hiddenss = entity_hiddens + tf.scatter_nd(tf.expand_dims(indices, 1), updated_hiddens - prev_states,
                                                        tf.shape(keys))
        return [encoded_sents, mask, keys, entity_hiddenss, tf.add(i, 1), iters]

    def body_2(encoded_sents, mask, keys, all_entity_hiddens, i, iters):
        indices = tf.where(mask[:, i])
        indices = tf.cast(tf.squeeze(indices, axis=1), tf.int32)
        curr_encoded_sents = tf.gather(encoded_sents[:, i, :], indices)
        curr_keys = tf.gather(keys, indices)
        prev_states = tf.gather(all_entity_hiddens[:, -1, :, :], indices)
        updated_hiddens = tf.expand_dims(entity_cell(curr_encoded_sents, prev_states, curr_keys), axis=1)
        all_entity_hiddenss = tf.concat([all_entity_hiddens,
                                        tf.scatter_nd(tf.expand_dims(indices, 1), updated_hiddens,
                                                      [batch_size, 1, key_num, entity_embedding_dim])], axis=1)
        return [encoded_sents, mask, keys, all_entity_hiddenss, tf.add(i, 1), iters]

    i = tf.constant(0)
    if return_last:
        encoded_sentss, maskk, keyss, entity_hiddenss, ii, iterss = tf.while_loop(cond, body_1,
                                                                            [encoded_sents, mask, keys,
                                                                             entity_hiddens, i, seq_length])
        to_return = entity_hiddenss
    else:
        # print("seq_length.get_shape()",seq_length.get_shape())
        encoded_sentss, maskk, keyss, all_entity_hiddenss, ii, iterss = tf.while_loop(cond, body_2,
                                                                                [encoded_sents, mask, keys,
                                                                                 all_entity_hiddens, i, seq_length]
                                                                                , shape_invariants=[
                encoded_sents.get_shape(), mask.get_shape(), keys.get_shape(),
                tf.TensorShape(
                    [encoded_sents.shape[0], None, keys.shape[1],
                     keys.shape[2]]),
                i.get_shape(), seq_length.get_shape()])
        to_return = all_entity_hiddenss

    return to_return


class BasicRecurrentEntityEncoder(tf.keras.Model):
    def __init__(self, embedding_matrix, max_entity_num=None, entity_embedding_dim=None, entity_cell=None, name=None,
                 **kwargs):
        if name is None:
            name = 'BasicRecurrentEntityEncoder'
        super().__init__(name=name)
        if entity_cell is None:
            if entity_embedding_dim is None:
                raise AttributeError('entity_embedding_dim should be given')
            if max_entity_num is None:
                raise AttributeError('max_entity_num should be given')
            entity_cell = EntityCell(max_entity_num=max_entity_num, entity_embedding_dim=entity_embedding_dim,
                                     name='entity_cell')
        self.entity_cell = entity_cell
        self.embedding_matrix = embedding_matrix
        print(type(entity_embedding_dim))
        self.fc1 = tf.keras.layers.Dense(512)
        self.fc2 = tf.keras.layers.Dense(entity_embedding_dim)
        self.sent_encoder_module = Sent_encoder()


    def get_embeddings(self, input):
        return tf.nn.embedding_lookup(self.embedding_matrix, input)

    def call(self, inputs, keyss, initial_entity_hidden_state=None,
             use_shared_keys=False, return_last=True, **kwargs):
        """
        Args:
            inputs: paragraph, paragraph mask in a list , paragraph of shape:[batch_size, max_sents_num, max_sents_len,
            keyss: entity keys of shape : [batch_size, max_entity_num, entity_embedding_dim]
            initial_entity_hidden_state
            use_shared_keys: bool
            return_last: entity_cell and encoded_sents of shape [batch_size, max_num_sent, sents_encoding_dim]
        """

        print("in call")
        if len(inputs) != 3:
            raise AttributeError('expected 3 inputs but', len(inputs), 'were given')
        prgrph, prgrph_mask, question = inputs
        prgrph=tf.convert_to_tensor(prgrph)
        prgrph_mask = tf.convert_to_tensor(prgrph_mask)
        question=tf.convert_to_tensor(question)
        batch_size = tf.shape(prgrph)[0]
        max_sent_num = tf.shape(prgrph)[1]
        prgrph_embeddings_0 = self.get_embeddings(prgrph)
        prgrph_embeddings_0 = tf.convert_to_tensor(prgrph_embeddings_0)
        prgrph_embeddings_1 = self.fc1(prgrph_embeddings_0)
        prgrph_embeddings = self.fc2(prgrph_embeddings_1)

        question_embedding_0=self.get_embeddings(question)
        question_embedding_1=self.fc1(question_embedding_0)
        question_embeddings=self.fc2(question_embedding_1)
        encoded_question=self.sent_encoder_module(question_embeddings)

        keys_embedding_0=tf.convert_to_tensor(self.get_embeddings(keyss))
        keys_embedding_1=self.fc1(keys_embedding_0)
        keys=self.fc2(keys_embedding_1)
        'prgrph_embeddings shape: [batch_size, max_sent_num, max_sent_len, embedding_dim]'
        encoded_sents = tf.zeros([batch_size, 1, prgrph_embeddings.shape[3]])
        print(encoded_sents.shape)
        # for i in range(max_sent_num):
        #     ''' to see which sentences are available '''
        #     indices = tf.where(prgrph_mask[:, i, 0])
        #     # print('indices shape encode:, indices.shape)
        #     indices = tf.cast(tf.squeeze(indices, axis=1), tf.int32)
        #     current_sents = tf.gather(prgrph_embeddings[:, i, :, :], indices)
        #     # print('current_sents_call shape:', current_sents.shape)
        #     curr_encoded_sents = tf.expand_dims(self.sent_encoder_module(current_sents), axis=1)
        #     encoded_sents = tf.concat([encoded_sents, curr_encoded_sents], axis=1)

        def cond(prgrph_mask, prgrph_embeddings, encoded_sents, i, max_sent_num):
            return tf.less(i, max_sent_num)

        def body(prgrph_mask, prgrph_embeddings, encoded_sents, i, max_sent_num):
            indices = tf.where(prgrph_mask[:, i, 0])
            indices = tf.cast(tf.squeeze(indices, axis=1), tf.int32)
            current_sents = tf.gather(prgrph_embeddings[:, i, :, :], indices)
            curr_encoded_sents = tf.expand_dims(self.sent_encoder_module(current_sents), axis=1)
            curr_encoded_sentss=tf.scatter_nd(tf.expand_dims(indices,1),curr_encoded_sents,[tf.shape(prgrph_embeddings)[0],1,tf.shape(prgrph_embeddings)[3]])
            tf.print("encoded_sents shape:",encoded_sents.shape)
            tf.print("curr_sents shape:",curr_encoded_sents.shape)
            encoded_sentss = tf.concat([encoded_sents, curr_encoded_sentss], axis=1)
            return [prgrph_mask, prgrph_embeddings, encoded_sentss, tf.add(i, 1), max_sent_num]

        i = tf.constant(0)
        print("prgrph_embeddings",tf.shape(prgrph_embeddings))
        print(encoded_sents.shape)
        with tf.control_dependencies([encoded_sents]):
            tf.print("encoded_sents",encoded_sents)
        prgrph_maskk, prgrph_embeddingss, encoded_sentss, ii, max_sent_numm = \
            tf.while_loop(cond, body, [prgrph_mask, prgrph_embeddings, encoded_sents, i, max_sent_num],
                          shape_invariants=[prgrph_mask.get_shape(), prgrph_embeddings.get_shape(),
                                            tf.TensorShape([prgrph_embeddings.get_shape()[0], None,
                                                            prgrph_embeddings.get_shape()[3]]),
                                            i.get_shape(), max_sent_num.get_shape()])
        print("encoded_sents", encoded_sentss)
        encoded_sentsss = encoded_sentss[:, 1:, :]
        sents_mask = prgrph_mask[:, :, 0]
        print("return_last_encoder", return_last)
        return self.entity_cell, simple_entity_network(entity_cell=self.entity_cell, inputs=[encoded_sentsss, sents_mask],
                                                       keys=keys,
                                                       initial_entity_hidden_state=initial_entity_hidden_state,
                                                       use_shared_keys=use_shared_keys,
                                                       return_last=return_last), encoded_question


class RecurrentEntitiyDecoder(tf.keras.layers.Layer):
    def __init__(self, embedding_matrix, entity_embedding_dim, vocab_size=None, softmax_layer=None, activation=None,
                 name=None, **kwargs):
        if name is None:
            name = 'RecurrentEntitiyDecoder'
        super().__init__(name=name)
        self.embedding_matrix = embedding_matrix
        self.embedding_dim = embedding_matrix.shape[1]
        self.entity_embedding_dim=entity_embedding_dim

        if softmax_layer is None:
            if vocab_size is None:
                raise AttributeError("softmax_layer and vocab_size can't be both None")
            self.softmax_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')

        if activation is None:
            activation = tf.nn.relu
        self.activation=activation

        self.sent_encoder_module = Sent_encoder()

        self.H = None

    def build(self, input_shape):
        self.entity_attn_matrix = K.random_normal_variable(shape=[self.entity_embedding_dim, self.entity_embedding_dim],
                                                           mean=0, scale=0.05, name='entity_attn_matrix')
        self.H = self.add_weight(name="H", shape=[self.entity_embedding_dim, self.entity_embedding_dim])

    def attention_entities(self, query, entities, keys_mask):
        '''
        Description:
            attention on entities

        Arges:
            inputs: query shape: [curr_prgrphs_num, embedding_dim]
                    entities shape: [curr_prgrphs_num, entities_num, entitiy_embedding_dim]
                    keys_mask shape: [curr_prgrphs_num, entities_num]
            output shape: [curr_prgrphs_num, entity_embedding_dim]
        '''

        print("attention_entities, entities shape: ", entities.shape)

        values = tf.identity(entities)
        query_shape = tf.shape(query)
        entities_shape = tf.shape(entities)
        batch_size = query_shape[0]
        seq_length = entities_shape[1]
        indices = tf.where(keys_mask)
        queries = tf.gather(query, indices[:, 0])
        entities = tf.boolean_mask(entities, keys_mask)
        # print(queries.shape)
        # print(self.entity_attn_matrix.shape)
        attention_logits = tf.reduce_sum(tf.multiply(tf.matmul(queries, self.entity_attn_matrix), entities), axis=-1)
        # print('attention logits:',attention_logits)
        # print('tf.where(memory_mask):',tf.where(memory_mask))
        attention_logits = tf.scatter_nd(tf.where(keys_mask), attention_logits, [batch_size, seq_length])
        attention_logits = tf.where(keys_mask, attention_logits, tf.fill([batch_size, seq_length], -float("Inf")))
        attention_coefficients = tf.nn.softmax(attention_logits, axis=1)
        attention = tf.expand_dims(attention_coefficients, -1) * values

        return tf.reduce_sum(tf.multiply(tf.expand_dims(tf.matmul(query, self.entity_attn_matrix), axis=1), entities),
                             axis=1)

    def get_embeddings(self, input):
        return tf.nn.embedding_lookup(self.embedding_matrix, input)

    def call(self, inputs, keys_mask, encoder_hidden_states=None,
             use_shared_keys=False,
             return_last=True, attention=False):
        """
        Args:
            inputs: [entity_hiddens, question] , keys_mask
            return: distribution on the guessed answer

        """

        if len(inputs) != 2:
            raise AttributeError('expected 2 inputs but', len(inputs), 'were given')

        entity_hiddens, encoded_question = inputs
        print("entity_hiddens",entity_hiddens,entity_hiddens.shape)
        entity_hiddens = tf.convert_to_tensor(entity_hiddens)
        keys_mask = tf.convert_to_tensor(keys_mask)

        u = self.attention_entities(encoded_question, entity_hiddens, keys_mask)
        print(u.shape)
        output = self.softmax_layer(self.activation(encoded_question + tf.matmul(u,self.H)))
        print("output shape",output.shape)
        return output


class Model(tf.keras.Model):
    def __init__(self, embedding_matrix, max_entity_num=None, entity_embedding_dim=None,
                 entity_cell=None, vocab_size=None, softmax_layer=None, activation=None):
        super().__init__()
#         alpha=self.add_weight(shape=[entity_embedding_dim],initializer=tf.constant_initializer(1.0),name='alpha')
#         alpha=tf.keras.backend.variable(value=tf.ones([entity_embedding_dim]),name='alpha')
#         if activation is None:
#           activation=partial(prelu,alpha=alpha)
        self.embedding_matrix=embedding_matrix
        self.encoder = BasicRecurrentEntityEncoder(embedding_matrix=embedding_matrix,
                                                   max_entity_num=max_entity_num,
                                                   entity_embedding_dim=entity_embedding_dim,
                                                   entity_cell=entity_cell)
        self.decoder = RecurrentEntitiyDecoder(embedding_matrix=embedding_matrix,
                                               entity_embedding_dim=entity_embedding_dim,
                                               vocab_size=vocab_size,
                                               softmax_layer=softmax_layer, activation=activation)

    def call(self, inputs, initial_entity_hidden_state=None,
             use_shared_keys=False, return_last=True):
        """
         inputs=[prgrph, prgrph_mask, question]
        """
        prgrph, prgrph_mask, question, keys, keys_mask = inputs
        prgrph=tf.cast(prgrph,tf.int32)
        question=tf.cast(question,tf.int32)
        keys_mask=tf.cast(keys_mask,tf.bool)
        keys=tf.cast(keys,tf.int32)
        print(prgrph.shape)
        print(keys_mask)
        entity_cell, entity_hiddens, encoded_question = self.encoder(inputs=[prgrph, prgrph_mask, question], keyss=keys,
                                                                      initial_entity_hidden_state=initial_entity_hidden_state,
                                                                      use_shared_keys=use_shared_keys,
                                                                      return_last=return_last)
        self.decoder.entity_cell = entity_cell
        output = self.decoder(inputs=[entity_hiddens, encoded_question], keys_mask=keys_mask)
        return output

import numpy as np
import tensorflow as tf
# import Model
import pickle
from tensorflow.contrib import seq2seq
from keras.utils import to_categorical



def calculate_loss(targets, outputs):
    """
    Args:
        inputs: outputs shape : [batch_size,max_sents_num*max_sents_len, vocab_size]
                lstm_targets shape : [batch_size, max_sents_num*max_sents_len]
                mask : [batch_size, max_sents_num*max_sents_len]

    """
    # one_hot_labels = tf.one_hot(lstm_targets, outputs.shape[1])
    # print('outpus shape:', outputs.shape, outputs)
    # print('one_hot_labels shape:', one_hot_labels.shape)
    # loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=outputs)
    # print('loss', loss)
    print('IN LOSS FUNCTION')
    print('outputs shape:', outputs.shape)
    print('output dtype:', outputs.dtype)
    print('targets dtype:', targets.dtype)
#     targetss=tf.one_hot(tf.squeeze(tf.cast(targets,tf.int32),axis=1),depth=30522)

    # print('mask dtype:', mask.dtype)
    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs,labels=targets)
    return loss


def train(prgrphs, prgrphs_mask, questions, answers, keys, keys_mask, embedding_matrix, max_entity_num,
          entity_embedding_dim, vocab_size, learning_rate, save_path, batch_size, validation_split, epochs):
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    session = tf.Session(config=config)
    tf.keras.backend.set_session(session)
    model = Model(embedding_matrix=embedding_matrix, max_entity_num=max_entity_num,
                        entity_embedding_dim=entity_embedding_dim,
                        vocab_size=vocab_size)
#     k=128
#     output=model([prgrphs[:k], prgrphs_mask[:k], questions[:k], keys[:k], keys_mask[:k]])
#     with tf.Session() as sess:
#         print("in session")
#         sess.run(tf.global_variables_initializer())
#         output=sess.run(output)
#         for i in range(k):
#           print(dictionary[output[i]],dictionary[answers[i,0]])
  
    def schedule(epoch_num, lr):
      new_lr=lr
      if epoch_num%25==0:
        new_lr=lr/2
      return new_lr
    
    num_batches=((1-validation_split)*prgrphs.shape[0])//batch_size
    train_loss=[]
    train_acc=[]
    val_loss=[]
    val_acc=[]
    
    def save_loss(batch,logs):
      nonlocal train_loss
      nonlocal train_acc
      if batch==0:
        train_loss=[]
        train_acc=[]
      train_loss.append(logs['loss'])
      train_acc.append(logs['acc'])
      
    def plot(loss, acc, rangee, mode, epoch=200):
        fig=plt.figure()
        ax1=fig.add_subplot(211)
        ax1.plot(np.arange(rangee),loss)
        ax1.set_xlabel(mode+" "+str(epoch))
        ax1.set_ylabel("loss")
        ax2=fig.add_subplot(212)
        ax2.plot(np.arange(rangee),acc)
        ax2.set_ylabel("accuracy")
        plt.show()
      
     
    start_time=time.time()
    
    def plot_train_loss(epoch,logs):
      nonlocal start_time
      end_time=time.time()
      print("time:",end_time-start_time)
      start_time=end_time
      val_loss.append(logs['val_loss'])
      val_acc.append(logs['val_acc'])
      plot(train_loss,train_acc,num_batches,'train',epoch)
      
      
    

    adam = tf.keras.optimizers.Adam(lr=learning_rate,clipnorm=40)
    model.compile(optimizer=adam,
                  loss='categorical_crossentropy',
                  metrics=['acc'])
    cp_callback = tf.keras.callbacks.ModelCheckpoint(save_path,
                                                     save_weights_only=True,
                                                     verbose=1)
    tn_callback=tf.keras.callbacks.TerminateOnNaN()
#     monitor_callback=tf.keras.callbacks.RemoteMonitor(root='http://localhost:8080', path='/publish/epoch/end/', field='data', headers=None, send_as_json=False)
    lr_callback=tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)
    lambda_callback=tf.keras.callbacks.LambdaCallback(on_batch_end=save_loss, on_epoch_end=plot_train_loss)
  
    # answerss=(tf.one_hot(tf.squeeze(tf.cast(answers,np.int32),axis=1),depth=30522)*(1-(10**-7))+10**-7)
#     answerss=(np.eye(30522)[np.squeeze(answers,axis=1)]+10**-7)*(1-(10**-7))

#     answerss=np.zeros([answers.shape[0],vocab_size],np.int32)
#     answerss[np.arange(answers.shape[0]),np.squeeze(answers)]=1

#     answerss=(answerss*(1-(10**-5)))+(10**-5)
    answerss=to_categorical(np.squeeze(answers,axis=1),num_classes=vocab_size)
    print(answerss[:3])
    history = model.fit(x=[prgrphs, prgrphs_mask, questions, keys, keys_mask], y=answerss, batch_size=batch_size,
                        validation_split=validation_split, epochs=epochs, 
                        callbacks=[cp_callback, tn_callback, lr_callback,
                                  lambda_callback],shuffle=True)
    plot(val_loss,val_acc,epochs,'validation')
    
   

    return history

with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_paragraphs.pkl","rb") as file:
  paragraphs=pickle.load(file)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_paragraphs_mask.pkl","rb") as file:
  paragraphs_mask=pickle.load(file)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_questions.pkl","rb") as file:
  questions=pickle.load(file)
with open("/content/drive/My Drive/Colab Notebooks/bAbI/q1_train_answers.pkl","rb") as file:
  answers=pickle.load(file)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/BERT2/embedding_matrix.pkl','rb') as file:
  embedding_matrix=pickle.load(file)
  
embedding_matrix[0]=np.zeros([1,768],np.float32)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/keys.pkl','rb') as file:
  keys=pickle.load(file)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/keys_mask.pkl','rb') as file:
  keys_mask=pickle.load(file)
with open('/content/drive/My Drive/Colab Notebooks/bAbI/config.pkl','rb') as file:
  config=pickle.load(file)
  
print("---------------")
paragraphs_num = config['paragraphs_num']
print(paragraphs_num)
max_sent_num = config['max_sent_num']
print(max_sent_num)
max_sent_len = config['max_sent_len']
print(max_sent_len)
batch_size=config['batch_size']
print(batch_size)
epochs_num=config['epochs_num']
print(epochs_num)
initial_lr=config['initial_lr']
print(initial_lr)
max_entity_num=config['max_entity_num']
print(max_entity_num)
vocab_size=config['vocab_size']
print(vocab_size)
embedding_dim=config['embedding_dim']
print(embedding_dim)
val_split=config['val_split']
print(val_split)
print("---------------")

# keys = np.random.normal(size=[paragraphs_num, 20, 100])
# keys_mask = np.ones([paragraphs_num, 20],np.bool)
    # print(paragraphs_num)
    # print(paragraphs[-1], paragraphs_mask[-1])
# batch_size=32
paragraphs=paragraphs[:paragraphs_num-(paragraphs_num % batch_size)]
paragraphs_mask=paragraphs_mask[:paragraphs_num-(paragraphs_num % batch_size)]
keys=keys[:paragraphs_num-(paragraphs_num % batch_size)]
keys_mask=keys_mask[:paragraphs_num-(paragraphs_num % batch_size)]
questions=questions[:paragraphs_num-(paragraphs_num % batch_size)]
answers=answers[:paragraphs_num-(paragraphs_num % batch_size)]

# print(embedding_matrix[0])

# paragraphs=paragraphs[:256]
# paragraphs_mask=paragraphs_mask[:256]
# keys=keys[:256]
# keys_mask=keys_mask[:256]
# questions=questions[:256]
# answers=answers[:256]
print(answers[:3])

train(paragraphs, paragraphs_mask, questions, answers, keys, keys_mask, embedding_matrix, max_entity_num, embedding_dim, vocab_size,
              initial_lr, "/content/drive/My Drive/Colab Notebooks/bAbI", batch_size, val_split, 200)
# for key in history.history.keys():
#   print(key)

